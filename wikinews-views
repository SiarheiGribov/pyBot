#!/usr/bin/env python
# -- coding: utf-8 --

import re
import time
from datetime import datetime, timedelta
import urllib.parse as quote
import requests
import login


exl = ["Медиа", "Служебная", "Обсуждение", "Участник", "Обсуждение участника", "Викиновости", "Обсуждение Викиновостей",
       "Файл", "Обсуждение файла", "MediaWiki", "Обсуждение MediaWiki", "Шаблон", "Обсуждение шаблона",
       "Справка", "Обсуждение справки", "Категория", "Обсуждение категории", "Портал", "Обсуждение портала",
       "Комментарии", "Обсуждение комментариев", "Модуль", "Обсуждение модуля", "Гаджет", "Обсуждение гаджета",
       "Определение гаджета", "Обсуждение определения гаджета", "Участница", "Обсуждение участницы"]
months = ["января", "февраля", "марта", "апреля", "мая", "июня", "июля", "августа", "сентября", "октября", "ноября",
          "декабря"]
page_header = "{{Самые популярные новости/Заголовок 2|$1 $2|$3}}"
page_top = "<table style='padding-left:5px;padding-right:5px;vertical-align:top'>"
page_line = "<tr><td>• [[$1]]</td><td style='text-align:right;vertical-align:top'>{{num|$2}}</td></tr>"
page_footer = "</table><noinclude>Данный список формируется на основе данных, полученных из " \
              "[https://wikimedia.org/api/rest_v1/ Wikimedia Rest API]. В количестве просмотров учитываются " \
              "посещения страниц как людьми, так и автоматизированными программами, поэтому следует иметь ввиду, " \
              "что данные могут быть подвержены искажениям, хотя в целом отражают реальный интерес к той или иной " \
              "новости. В статистике учитываются просмотры за период с 00:00:00 до 23:59:59 по [[Всемирное " \
              "координированное время|Всемирному координированному времени]] (UTC). Статистика обновляется " \
              "автоматически по окончанию дня.</div>\n\n[[Категория:Викиновости:Шаблоны|Самые популярные " \
              "новости]]\n[[Категория:Викиновости:Статистика и прогнозы|Самые популярные новости]]\n</noinclude>"
ua = {"User-agent": "pyBot/pageviews (toolforge/iluvatarbot; iluvatar@tools.wmflabs.org) requests"}
yesterday = datetime.now() - timedelta(1)
yesterday_slash = "{0}{1}{2}".format(str(yesterday.strftime("%Y")), str(yesterday.strftime("%m")),
                                     str(yesterday.strftime("%d")))
API_url = "https://wikimedia.org/api/rest_v1/metrics/pageviews/top/ru.wikinews/all-access/{0}/{1}/{2}".format(
    str(yesterday.strftime("%Y")), str(yesterday.strftime("%m")), str(yesterday.strftime("%d")))
API_url_per_page = "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/ru.wikinews/" \
                   "all-access/user/{0}/daily/" + yesterday_slash + "/" + yesterday_slash
API_url_wikinews = "https://ru.wikinews.org/w/api.php"


def get_data():
    try:
        r = requests.get(url=API_url, headers=ua).json()["items"][0]["articles"]
    except Exception as e:
        print("Get data error: {0}".format(e))
        time.sleep(30)
        get_data()
    else:
        handler(r)


def handler(views):
    top = {}
    i = 0
    max_views = 0
    for article in views:
        name = str(article["article"].replace("_", " "))
        check = True
        for e in exl:
            if bool(re.search(r'(' + e + ':)', name)):
                check = False
        if check and name != "Заглавная страница" and not name.startswith("Лента новостей "):
            data = {"action": "query", "titles": name, "redirects": "", "format": "json", "utf8": 1}
            try:
                time.sleep(3)
                r = requests.post(API_url_wikinews, data=data, headers=ua).json()
                # Если это редирект
                if "redirects" in r["query"]:
                    # Если страница, на которую он ссылается, уже в топе
                    top_check = False
                    for top_el in top:
                        if r["query"]["redirects"][0]["to"] == top[top_el]["name"]:
                            top[top_el]["views"] += article["views"]
                            top_check = True
                    # Если страницы, на которую он ссылается, ещё нет в топе
                    if not top_check:
                        url_quoted = quote.quote_plus(r["query"]["redirects"][0]["to"].replace(" ", "_"))
                        time.sleep(3)
                        r2 = requests.get(API_url_per_page.format(url_quoted), headers=ua).json()
                        # если 0 просмотров
                        r2 = 0 if "items" not in r2 else r2["items"][0]["views"]
                        top[i] = {}
                        top[i]["name"] = r["query"]["redirects"][0]["to"]
                        top[i]["views"] = article["views"] + r2
                        i += 1
                #  если это не редирект
                else:
                    top_check = False
                    for top_el in top:
                        if name == top[top_el]["name"]:
                            top_check = True
                    if not top_check:
                        top[i] = {}
                        top[i]["name"] = name
                        top[i]["views"] = article["views"]
                        i += 1
            except Exception as e:
                print("Get page info error: {0}".format(e))
            else:
                # Минимальное число просмотров в валидном (первые 11) топе. Если его невозможно перебить
                # (редирект + страница, то есть x2 в теории), дальше не идём
                if i == 11:
                    max_views = article["views"]
                if i >= 11 and article["views"] * 2 <= max_views:
                    break

    page = page_header.replace("$1", re.sub("^0", "", yesterday.strftime("%d"))).replace("$2", months[
        int(yesterday.strftime("%m")) - 1]).replace("$3", yesterday.strftime("%Y")) + "\n" + page_top + "\n"
    top_res = []
    for res in top:
        top_res.append(top[res])
    top_res = sorted(top_res, key=lambda x: x["views"], reverse=True)[:11]
    for t in top_res:
        if str(t["name"]).startswith("Категория:"):
            t["name"] = ":{0}|{1}".format(str(t["name"]), str(t["name"]).replace("Категория:", ""))
        page += page_line.replace("$1", str(t["name"])).replace("$2", str(t["views"])) + "\n"
    page += page_footer

    post(page)


def post(page):
    token, cookies = login.login(server="ru.wikinews")
    params = {
        "action": "edit", "format": "json", "utf8": "1", "title": "Шаблон:Самые популярные новости 2", "text": page,
        "summary": "Обновление статистики", "bot": 1, "token": token
    }
    requests.post(url=API_url_wikinews, data=params, cookies=cookies)


get_data()
